{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1bf3b5",
   "metadata": {},
   "source": [
    "# Load transformed_full.csv into SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1139c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " full_data saved to loaded/full_data.db\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd             # For handling data\n",
    "import sqlite3                 # For connecting to SQLite databases\n",
    "import os                     # For working with directories\n",
    "\n",
    "# Make sure the target folder exists\n",
    "os.makedirs(\"loaded\", exist_ok=True)\n",
    "\n",
    "# Load the transformed full dataset from CSV\n",
    "full_df = pd.read_csv(\"transformed/transformed_full.csv\")\n",
    "\n",
    "# Create a SQLite connection for full_data\n",
    "full_conn = sqlite3.connect(\"loaded/full_data.db\")\n",
    "\n",
    "# Save the DataFrame as a table named 'full_data' inside the database\n",
    "full_df.to_sql(\"full_data\", full_conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Confirm saving is complete\n",
    "print(\" full_data saved to loaded/full_data.db\")\n",
    "\n",
    "# Close the connection to free resources\n",
    "full_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02725282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   order_id customer_name product  quantity  unit_price  order_date  \\\n",
      "0         1         Diana  Tablet  1.958904       500.0  2024-01-20   \n",
      "1         2           Eve  Laptop  1.958904       250.0  2024-04-29   \n",
      "2         3       Charlie  Laptop  2.000000       250.0  2024-01-08   \n",
      "3         4           Eve  Laptop  2.000000       750.0  2024-01-07   \n",
      "4         5           Eve  Tablet  3.000000       500.0  2024-03-07   \n",
      "\n",
      "          region  total_price customer_tier purchase_day  \n",
      "0          South   979.452055        Silver     Saturday  \n",
      "1          North   489.726027        Bronze       Monday  \n",
      "2  Not Specified   500.000000        Bronze       Monday  \n",
      "3           West  1500.000000          Gold       Sunday  \n",
      "4          South  1500.000000          Gold     Thursday  \n"
     ]
    }
   ],
   "source": [
    "# Reconnect to the database and load table into a DataFrame\n",
    "conn = sqlite3.connect(\"loaded/full_data.db\")\n",
    "preview_full = pd.read_sql(\"SELECT * FROM full_data\", conn)\n",
    "\n",
    "# Show the first 5 rows\n",
    "print(preview_full.head())\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a594fff3",
   "metadata": {},
   "source": [
    "#  Load incremental_data into SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a011a010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "incremental_data saved to loaded/incremental_data.db\n",
      "   order_id customer_name product  quantity  unit_price  order_date   region  \\\n",
      "0       101         Alice  Laptop       1.5       900.0  2024-05-09  Central   \n",
      "1       102       Unknown  Laptop       1.0       300.0  2024-05-07  Central   \n",
      "2       103       Unknown  Laptop       1.0       600.0  2024-05-04  Central   \n",
      "3       104       Unknown  Tablet       1.5       300.0  2024-05-26  Central   \n",
      "4       105         Heidi  Tablet       2.0       600.0  2024-05-21    North   \n",
      "\n",
      "   total_price purchase_period  is_weekend  \n",
      "0       1350.0         Morning           0  \n",
      "1        300.0         Morning           0  \n",
      "2        600.0         Morning           1  \n",
      "3        450.0         Morning           1  \n",
      "4       1200.0         Morning           0  \n"
     ]
    }
   ],
   "source": [
    "# Load the transformed incremental dataset from CSV\n",
    "incr_df = pd.read_csv(\"transformed/transformed_incremental.csv\")\n",
    "\n",
    "# Create a SQLite connection for incremental_data\n",
    "incr_conn = sqlite3.connect(\"loaded/incremental_data.db\")\n",
    "\n",
    "# Save the DataFrame as a table named 'incremental_data' in the database\n",
    "incr_df.to_sql(\"incremental_data\", incr_conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "# Confirm saving is complete\n",
    "print(\"incremental_data saved to loaded/incremental_data.db\")\n",
    "\n",
    "# Close the connection\n",
    "incr_conn.close()\n",
    "\n",
    "# Reconnect to the database\n",
    "conn = sqlite3.connect(\"loaded/incremental_data.db\")\n",
    "\n",
    "# Run a simple SQL query to preview the data\n",
    "print(pd.read_sql(\"SELECT * FROM incremental_data LIMIT 5\", conn))\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d43f2",
   "metadata": {},
   "source": [
    "The loading process is the final and crucial phase of the ETL pipeline, where all cleaned and transformed data is securely stored in structured, accessible formats. By saving the datasets into SQLite databases, we ensure the data can be easily queried, reused, and integrated with various downstream tools and workflows. This step facilitates efficient data retrieval and supports scalability, making the data suitable for use in dashboards, machine learning pipelines, or business intelligence reports.\n",
    "\n",
    "In this project, the transformed datasets were loaded as follows: the transformed_full.csv file was imported into loaded/full_data.db under the table name full_data, and the transformed_incremental.csv file was saved into loaded/incremental_data.db under the table incremental_data. This organized storage approach not only preserves data integrity but also allows seamless access and analysis using SQL queries or data tools like Pandas, ensuring readiness for real-world deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
